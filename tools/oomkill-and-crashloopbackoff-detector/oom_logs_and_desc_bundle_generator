#!/opt/homebrew/bin/bash

set -e

# Default values
POD_NAME=""
OUTPUT_DIR="output"
CODEOWNERS_DIR=""      # If set, look up namespace owners from CODEOWNERS and GitLab (glab)
CODEOWNERS_TEMP_DIR="" # If we clone konflux-release-data, we remove this on exit
KONFLUX_RELEASE_DATA_REPO="git@gitlab.cee.redhat.com:releng/konflux-release-data.git"

# Ordinal suffix for day: 1st, 2nd, 3rd, 4th, ..., 11th, 21st, 22nd, 23rd, etc.
day_ordinal() {
    local d="$1"
    [[ -z "$d" || ! "$d" =~ ^[0-9]+$ ]] && echo "${d}th" && return
    case "$((d % 100))" in
        11|12|13) echo "${d}th" ;;
        *) case "$((d % 10))" in
               1) echo "${d}st" ;;
               2) echo "${d}nd" ;;
               3) echo "${d}rd" ;;
               *) echo "${d}th" ;;
           esac ;;
    esac
}

# Extract date label from date-wise CSV filename: oom_results_28-Jan-2026_14-55-05-EDT.csv -> 28th-Jan-2026
date_label_from_csv_basename() {
    local base="$1"
    # Match oom_results_DD-Mon-YYYY_*
    if [[ "$base" =~ ^oom_results_([0-9]{2})-([A-Za-z]{3})-([0-9]{4})_ ]]; then
        local dd="${BASH_REMATCH[1]}"
        local mon="${BASH_REMATCH[2]}"
        local yyyy="${BASH_REMATCH[3]}"
        local ord
        ord=$(day_ordinal "$((10#${dd}))")
        echo "${ord}-${mon}-${yyyy}"
    fi
}

# Extract date key from filename (DD-Mon-YYYY) for grouping; for oom_results.csv use file mtime
date_key_from_csv_basename() {
    local base="$1"
    local csv_path="$2"
    if [[ "$base" =~ ^oom_results_([0-9]{2})-([A-Za-z]{3})-([0-9]{4})_ ]]; then
        echo "${BASH_REMATCH[1]}-${BASH_REMATCH[2]}-${BASH_REMATCH[3]}"
        return
    fi
    if [[ "$base" == "oom_results.csv" && -n "$csv_path" && -f "$csv_path" ]]; then
        local mtime
        if mtime=$(stat -f %m "$csv_path" 2>/dev/null); then
            date -r "$mtime" "+%d-%b-%Y" 2>/dev/null || true
        elif mtime=$(stat -c %Y "$csv_path" 2>/dev/null); then
            date -d "@$mtime" "+%d-%b-%Y" 2>/dev/null || true
        fi
    fi
}

# Get owner @usernames for (cluster, namespace) from CODEOWNERS. Output: @user1 @user2 ...
get_owners_for_namespace() {
    local codeowners_dir="$1"
    local cluster="$2"
    local namespace="$3"
    [[ -z "$codeowners_dir" || ! -d "$codeowners_dir" ]] && return
    grep -h "/tenants-config/cluster/${cluster}/" "${codeowners_dir}/CODEOWNERS" "${codeowners_dir}/staging/CODEOWNERS" 2>/dev/null | grep -v '^#' | grep "${namespace}" | head -1 | grep -oE '@[a-zA-Z0-9_.-]+' | sort -u
}

# Get "Name <email>" for a GitLab username via glab. Falls back to "(no public email)" or "(lookup failed)".
get_user_display() {
    local username="$1"
    [[ -z "$username" ]] && echo "(unknown)" && return
    local resp
    resp=$(glab api "users?username=${username}" 2>/dev/null)
    local name email
    name=$(echo "$resp" | python3 -c "import sys,json; d=json.load(sys.stdin); d=d[0] if isinstance(d,list) and d else d; print((d.get('name') or '').strip() if d else '')" 2>/dev/null)
    email=$(echo "$resp" | python3 -c "import sys,json; d=json.load(sys.stdin); d=d[0] if isinstance(d,list) and d else d; e=d.get('public_email') if d else None; print(e if e and str(e) != 'None' else '')" 2>/dev/null)
    if [[ -z "$name" ]]; then
        echo "@${username} (lookup failed)"
        return
    fi
    if [[ -n "$email" ]]; then
        echo "${name} <${email}>"
    else
        echo "${name} (no public email)"
    fi
}

# Build curl auth options from env: REMOTE_USER/REMOTE_TOKEN or JENKINS_USER/JENKINS_TOKEN; else .netrc is used by curl
build_curl_auth() {
    CURL_AUTH_ARRAY=()
    if [[ -n "${REMOTE_USER:-}" && -n "${REMOTE_TOKEN:-}" ]]; then
        CURL_AUTH_ARRAY=(-u "${REMOTE_USER}:${REMOTE_TOKEN}")
    elif [[ -n "${JENKINS_USER:-}" && -n "${JENKINS_TOKEN:-}" ]]; then
        CURL_AUTH_ARRAY=(-u "${JENKINS_USER}:${JENKINS_TOKEN}")
    fi
}

# Recursively download directory listing from base_url/path_prefix into dest_dir.
# base_url has no trailing slash; path_prefix may be "" or "subdir/" (trailing slash for dirs).
download_directory_from_url() {
    local base_url="$1"
    local path_prefix="$2"
    local dest_dir="$3"
    local full_url index_html dir_path
    if [[ -z "$path_prefix" ]]; then
        full_url="${base_url}/"
    else
        full_url="${base_url}/${path_prefix}"
    fi
    index_html=$(mktemp 2>/dev/null || mktemp -t oom_index)
    trap 'rm -f "$index_html"' RETURN
    if ! curl -s -f "${CURL_AUTH_ARRAY[@]}" -o "$index_html" "$full_url"; then
        echo "Warning: could not fetch directory listing: $full_url" >&2
        return 0
    fi
    grep -oE 'href="[^"]*"' "$index_html" 2>/dev/null | sed 's/^href="//;s/"$//' | while IFS= read -r href; do
        [[ -z "$href" ]] && continue
        href="${href#/}"
        [[ -z "$href" ]] && continue
        [[ "$href" == "#" || "$href" == "?"* || "$href" == "javascript:"* ]] && continue
        [[ "$href" == "../"* || "$href" == ".." ]] && continue
        [[ "$href" == "http://"* || "$href" == "https://"* ]] && continue
        if [[ "$href" == */ ]]; then
            download_directory_from_url "$base_url" "${path_prefix}${href}" "$dest_dir"
        else
            dir_path="${dest_dir}/${path_prefix}"
            mkdir -p "$(dirname "${dir_path}${href}")"
            if curl -s -f "${CURL_AUTH_ARRAY[@]}" -o "${dir_path}${href}" "${base_url}/${path_prefix}${href}"; then
                echo "Downloaded: ${path_prefix}${href}"
            fi
        fi
    done
}

# Download full remote directory (HTML index) into dest_dir. base_url has no trailing slash.
download_from_url() {
    local base_url="$1"
    local dest_dir="$2"
    mkdir -p "$dest_dir"
    build_curl_auth
    download_directory_from_url "$base_url" "" "$dest_dir"
}

# Upload all files under local_dir/tarballs/ to base_url/tarballs/ (PUT). base_url has no trailing slash.
upload_tarballs_to_url() {
    local base_url="$1"
    local local_dir="$2"
    local tarball_dir="${local_dir}/tarballs"
    local upload_base="${base_url}/tarballs"
    build_curl_auth
    if [[ ! -d "$tarball_dir" ]]; then
        echo "No tarballs to upload." >&2
        return 0
    fi
    for f in "$tarball_dir"/*.tgz "$tarball_dir"/*.tar.gz; do
        [[ -f "$f" ]] || continue
        name=$(basename "$f")
        if curl -s -f "${CURL_AUTH_ARRAY[@]}" -T "$f" "${upload_base}/${name}"; then
            echo "Uploaded: ${upload_base}/${name}"
        else
            echo "Warning: upload failed: ${upload_base}/${name}" >&2
        fi
    done
}

print_help() {
    cat << EOF
Usage: $0 -p POD_NAME [OPTIONS]

Required Arguments:
  -p, --pod-name POD_NAME    Pod name (or substring) to match; e.g. image-controller-image-pruner-cronjob
                            matches image-controller-image-pruner-cronjob-29439360-r9np8 (must appear in date-wise CSVs)

Options:
  -d, --output-dir DIR      Directory containing date-wise oom_results_*.csv files (default: output).
                            If DIR starts with http:// or https://, treats it as a URL: downloads the
                            directory (HTML index listing) into a temp dir, runs the generator, then
                            uploads new tarballs to <URL>/tarballs/. Auth: REMOTE_USER + REMOTE_TOKEN
                            (or JENKINS_USER + JENKINS_TOKEN), or .netrc.
  -c, --codeowners-dir DIR  Path to konflux-release-data (contains CODEOWNERS, staging/CODEOWNERS).
                            If not set, script clones the repo to a temp dir, uses it, then deletes it.
                            Report always includes namespace owners (name + email via glab) when available.
  -h, --help                Show this help message

Behavior:
  When -d is a local directory: uses it as-is. When -d is an http(s) URL: downloads the
  directory (HTML index) into a temp dir, runs steps 1â€“3 below, then uploads new tarballs to <URL>/tarballs/.
  1. Scans all date-wise CSV files in OUTPUT_DIR (oom_results_<DD>-<Mon>-<YYYY>_*.csv).
  2. For the given POD, reports how many OOMKilled and CrashLoopBackOff instances were
     detected and on which days.
  3. Creates one tarball per (type, date), with pod name in the filename, under output/tarballs/, e.g.:
     output/tarballs/OOMKilled-image-controller-image-pruner-cronjob-instance-28th-Jan-2026.tgz
     output/tarballs/CrashLoopBackOff-image-controller-image-pruner-cronjob-instance-29th-Jan-2026.tgz
     Find all tarballs for a pod: ls output/tarballs/*<pod-name>*.tgz
     Each tarball contains logs and descriptions for that pod on that date.

Examples:
  $0 -p loki-ingester-zone-a-0
  $0 --pod-name audit-exporter-2fzlc -d output
  $0 -p oom-stress-retry -c /path/to/konflux-release-data   # include namespace owners (name + email via glab)
  $0 -p prefetch-dependencies -d output -c /path/to/konflux-release-data   # all options combined
  REMOTE_USER=user REMOTE_TOKEN=token $0 -p my-pod -d https://jenkins.example.com/job/oom/ws/artifacts   # URL: download, generate, upload tarballs

EOF
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -p|--pod-name)
            POD_NAME="$2"
            shift 2
            ;;
        -d|--output-dir)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        -c|--codeowners-dir)
            CODEOWNERS_DIR="$2"
            shift 2
            ;;
        -h|--help)
            print_help
            exit 0
            ;;
        *)
            echo "ERROR: Unknown option: $1" >&2
            print_help
            exit 1
            ;;
    esac
done

if [[ -z "$POD_NAME" ]]; then
    echo "ERROR: -p/--pod-name is required" >&2
    print_help
    exit 1
fi

REMOTE_URL=""
if [[ "$OUTPUT_DIR" == http://* || "$OUTPUT_DIR" == https://* ]]; then
    REMOTE_URL="${OUTPUT_DIR%/}"
    TMP_DOWNLOAD=$(mktemp -d 2>/dev/null || mktemp -d -t oom_bundle_download)
    echo "Downloading from $REMOTE_URL into $TMP_DOWNLOAD ..."
    download_from_url "$REMOTE_URL" "$TMP_DOWNLOAD"
    OUTPUT_DIR="$TMP_DOWNLOAD"
else
    if [[ ! -d "$OUTPUT_DIR" ]]; then
        echo "ERROR: Output directory not found: $OUTPUT_DIR" >&2
        exit 1
    fi
    # Resolve OUTPUT_DIR so we always use a consistent path (handles relative paths)
    OUTPUT_DIR="$(cd "$OUTPUT_DIR" && pwd)"
fi

# Find date-wise CSVs (oom_results_DD-Mon-YYYY_*.csv) and main oom_results.csv
DATEWISE_CSVS=()
while IFS= read -r -d '' f; do
    DATEWISE_CSVS+=("$f")
done < <(find "$OUTPUT_DIR" -maxdepth 1 -name 'oom_results_*_*.csv' -print0 2>/dev/null | sort -z)

# Also include main oom_results.csv (use its mtime date as the date bucket)
MAIN_CSV="${OUTPUT_DIR}/oom_results.csv"
if [[ -f "$MAIN_CSV" ]]; then
    DATEWISE_CSVS+=("$MAIN_CSV")
fi

if [[ ${#DATEWISE_CSVS[@]} -eq 0 ]]; then
    echo "ERROR: No CSV files found in $OUTPUT_DIR (expected oom_results.csv or oom_results_<DD>-<Mon>-<YYYY>_<time>.csv)" >&2
    exit 1
fi

# Collect per (type, date): list of "desc_file|log_file"; and per (type, date, cluster, namespace): count
declare -A FILES_BY_KEY    # "type|date_key" -> "desc1|log1 desc2|log2 ..."
declare -A COUNT_BY_NS     # "type|date_key|cluster|namespace" -> count

for csv in "${DATEWISE_CSVS[@]}"; do
    base=$(basename "$csv")
    date_key=$(date_key_from_csv_basename "$base" "$csv")
    [[ -z "$date_key" ]] && continue

    # CSV columns: 1=cluster, 2=namespace, 3=pod, 4=type, 5=application, 6=component, 7=timestamps, 8=sources, 9=description_file, 10=pod_log_file, 11=time_range
    while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        cluster=$(echo "$line" | awk -F',' '{print $1}')
        namespace=$(echo "$line" | awk -F',' '{print $2}')
        type=$(echo "$line" | awk -F',' '{print $4}')
        desc=$(echo "$line" | awk -F',' '{print $9}')
        log=$(echo "$line" | awk -F',' '{print $10}')
        [[ -z "$type" || -z "$desc" || -z "$log" ]] && continue
        # Normalize type
        case "$(echo "$type" | tr '[:upper:]' '[:lower:]')" in
            oomkilled) type="OOMKilled" ;;
            crashloopbackoff) type="CrashLoopBackOff" ;;
            *) continue ;;
        esac
        key="${type}|${date_key}"
        FILES_BY_KEY[$key]+="${desc}|${log} "
        key_ns="${type}|${date_key}|${cluster}|${namespace}"
        COUNT_BY_NS[$key_ns]=$((${COUNT_BY_NS[$key_ns]:-0} + 1))
    # Match pod by substring: user can pass base name (e.g. image-controller-image-pruner-cronjob)
    # and we match full pod names (e.g. image-controller-image-pruner-cronjob-29439360-r9np8)
    done < <(awk -F',' -v pod="$POD_NAME" 'NR>1 && index($3, pod) > 0 {print $0}' "$csv" 2>/dev/null)
done

# If -c was not passed, clone konflux-release-data to a temp dir so we can still show owner name+email
if [[ -z "$CODEOWNERS_DIR" || ! -d "$CODEOWNERS_DIR" ]]; then
    CODEOWNERS_TEMP_DIR=$(mktemp -d 2>/dev/null || mktemp -d -t oom-bundle)
    if git clone --depth 1 -q "$KONFLUX_RELEASE_DATA_REPO" "$CODEOWNERS_TEMP_DIR" 2>/dev/null; then
        CODEOWNERS_DIR="$CODEOWNERS_TEMP_DIR"
    else
        rm -rf "$CODEOWNERS_TEMP_DIR" 2>/dev/null || true
        CODEOWNERS_TEMP_DIR=""
    fi
fi

# Report: counts per (type, date, cluster, namespace) with optional owner name+email
echo "=============================================="
echo "Report for pod: $POD_NAME"
echo "=============================================="

total_oom=0
total_crash=0
for type in OOMKilled CrashLoopBackOff; do
    # Get all keys for this type from COUNT_BY_NS (format: type|date_key|cluster|namespace)
    keys_for_type=()
    for key in "${!COUNT_BY_NS[@]}"; do
        [[ "$key" != "${type}|"* ]] && continue
        keys_for_type+=("$key")
    done
    if [[ ${#keys_for_type[@]} -eq 0 ]]; then
        echo "${type}: 0 instances (no occurrences in date-wise CSVs)"
        continue
    fi
    # Sort keys by date_key then cluster then namespace
    while IFS= read -r key; do
        [[ -z "$key" ]] && continue
        count="${COUNT_BY_NS[$key]:-0}"
        rest="${key#*|}"
        date_key="${rest%%|*}"
        rest2="${rest#*|}"
        cluster="${rest2%%|*}"
        namespace="${rest2#*|}"
        owner_part=""
        if [[ -n "$CODEOWNERS_DIR" && -d "$CODEOWNERS_DIR" ]]; then
            owners=$(get_owners_for_namespace "$CODEOWNERS_DIR" "$cluster" "$namespace")
            if [[ -n "$owners" ]]; then
                owner_displays=()
                for u in $owners; do
                    username="${u#@}"
                    owner_displays+=("$(get_user_display "$username")")
                done
                owner_str=$(printf '%s, ' "${owner_displays[@]}" | sed 's/, $//')
                owner_part=" is owned by \"${owner_str}\""
            else
                owner_part=" (no owner in CODEOWNERS)"
            fi
        else
            owner_part=" (no CODEOWNERS repo available)"
        fi
        echo "${type}: ${count} instance(s) on ${date_key}, Namespace: ${namespace} (cluster: ${cluster})${owner_part}"
        if [[ "$type" == "OOMKilled" ]]; then
            ((total_oom += count)) || true
        else
            ((total_crash += count)) || true
        fi
    done < <(printf '%s\n' "${keys_for_type[@]}" | sort -t'|' -k2,2 -k3,3 -k4,4)
done

echo "=============================================="

if [[ $total_oom -eq 0 && $total_crash -eq 0 ]]; then
    echo "No OOMKilled or CrashLoopBackOff instances found for pod '$POD_NAME' in date-wise CSVs. Exiting."
    exit 0
fi

# Sanitize pod name for use in tarball filenames (alphanumeric, hyphen, underscore only)
POD_SANITIZED=$(echo "$POD_NAME" | sed 's/[^A-Za-z0-9_.-]/_/g' | sed 's/__*/_/g' | sed 's/^_\|_$//g')
[[ -z "$POD_SANITIZED" ]] && POD_SANITIZED="pod"

# Create one tarball per (type, date), with pod name in filename so different pods don't overwrite.
# Write all tarballs under <output_dir>/tarballs/ to avoid cluttering the main output directory.
TARBALL_DIR="${OUTPUT_DIR}/tarballs"
mkdir -p "$TARBALL_DIR"
TMP_BASE=$(mktemp -d)
trap 'rm -rf "$CODEOWNERS_TEMP_DIR" "$TMP_BASE" "${TMP_DOWNLOAD:-}"' EXIT

for key in "${!FILES_BY_KEY[@]}"; do
    type="${key%%|*}"
    date_key="${key#*|}"
    files_str="${FILES_BY_KEY[$key]:-}"
    [[ -z "$files_str" ]] && continue
    # date_key is DD-Mon-YYYY; we need ordinal label for tarball name
    if [[ "$date_key" =~ ^([0-9]{2})-([A-Za-z]{3})-([0-9]{4})$ ]]; then
        dd="${BASH_REMATCH[1]}"
        mon="${BASH_REMATCH[2]}"
        yyyy="${BASH_REMATCH[3]}"
        ord=$(day_ordinal "$((10#${dd}))")
        date_label="${ord}-${mon}-${yyyy}"
    else
        date_label="$date_key"
    fi
    # Include pod name in tarball filename so you can find with: ls tarballs/*image-controller-image-pruner-cronjob*.tgz
    tarball_name="${type}-${POD_SANITIZED}-instance-${date_label}.tgz"
    tarball_path="${TARBALL_DIR}/${tarball_name}"
    tmpdir="${TMP_BASE}/${type}-${date_key}"
    mkdir -p "$tmpdir"
    for pair in $files_str; do
        desc="${pair%%|*}"
        log="${pair#*|}"
        if [[ -f "$desc" ]]; then
            cp "$desc" "$tmpdir/"
        else
            echo "Warning: description file not found (skipped): $desc" >&2
        fi
        if [[ -f "$log" ]]; then
            cp "$log" "$tmpdir/"
        else
            echo "Warning: log file not found (skipped): $log" >&2
        fi
    done
    # Only create tarball if at least one file was copied (avoid empty tarballs)
    if [[ -n "$(find "$tmpdir" -maxdepth 1 -type f 2>/dev/null)" ]]; then
        tar -czf "$tarball_path" -C "$tmpdir" .
        echo "Created: $tarball_path"
    else
        echo "Skipped (no files found): $tarball_path" >&2
    fi
done

echo "Done. Tarballs written to $OUTPUT_DIR/tarballs/"
if [[ -n "$REMOTE_URL" ]]; then
    echo "Uploading tarballs to $REMOTE_URL/tarballs/ ..."
    upload_tarballs_to_url "$REMOTE_URL" "$OUTPUT_DIR"
fi
